{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0eaeec-ddf5-41a8-ac19-c2498d6da755",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5014cdb8-0fbe-4e60-8b46-094510ef25a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# directories\n",
    "INTERACTIVE_READER = True\n",
    "TOTAL_IMAGE_SNAPSHOTS = 10\n",
    "LOGS_DIRECTORY = os.path.join(\"__logs__\")\n",
    "DATA_DIRECTORY = os.path.join(\"__data__\")\n",
    "TEMP_DIRECTORY = os.path.join(\"__temp__\")\n",
    "MODELS_DIRECTORY = os.path.join(\"public/models\")\n",
    "FACES_DIRECTORY = os.path.join(DATA_DIRECTORY, \"faces\")\n",
    "\n",
    "# models\n",
    "DNN_MODEL_PATH = os.path.join(MODELS_DIRECTORY, \"opencv_face_detector_uint8.pb\")\n",
    "DNN_MODEL_CONFIG_PATH = os.path.join(MODELS_DIRECTORY, \"opencv_face_detector.pbtxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897fe9a-007f-403f-9d94-f8e462efa872",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b65777f-bf37-4f37-9b3a-b7abd8083dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "\n",
    "# get the face detector ( using opencv's dnn model)\n",
    "def get_detector():\n",
    "    net = cv2.dnn.readNet(DNN_MODEL_PATH, DNN_MODEL_CONFIG_PATH)\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "    detector = cv2.dnn_DetectionModel(net)\n",
    "    detector.setInputParams(size=(200, 200), scale=1, mean=(104, 117, 123), swapRB=True)\n",
    "    \n",
    "    return detector\n",
    "\n",
    "\n",
    "# utility function to show progress size\n",
    "def reporthook(count, block_size, total_size):\n",
    "    percent = min((count * block_size * 100 / total_size), 100)\n",
    "    sys.stdout.write(\"\\r%.2f%%\" % percent)\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77feb04-5fcb-42d8-baf5-36556056506d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522b3a4d-3bbb-4b12-a17a-4dae4b0f7840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "\n",
    "for directory in [TEMP_DIRECTORY, LOGS_DIRECTORY, DATA_DIRECTORY, MODELS_DIRECTORY, FACES_DIRECTORY]:\n",
    "    if not os.path.isdir(directory):\n",
    "        print(f\"creating `{directory}` directory.\")\n",
    "        os.mkdir(directory)\n",
    "\n",
    "for file in [DNN_MODEL_PATH, DNN_MODEL_CONFIG_PATH]:\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"downloading `{file}` from github...\")\n",
    "        request.urlretrieve(\n",
    "            f\"https://raw.githubusercontent.com/sharmapukar217/minor-project-resources/main/{file}\",\n",
    "            file,\n",
    "            reporthook,\n",
    "        )\n",
    "        print(f\"\\nfile saved to `{file}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7139889-d1f9-4224-8a04-0ecc867b13be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup faces ( for random subjects/faces )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b461428d-685c-40dc-bc82-f16f0b594e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating face datas...\n",
      "extracting and processing faces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [00:27<00:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "detector = get_detector()\n",
    "if len(os.listdir(FACES_DIRECTORY)) == 0:\n",
    "    file = \"archive.zip\"\n",
    "    print(\"creating face datas...\")\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"downloading `{file}` file from github...\")\n",
    "        request.urlretrieve(f\"https://raw.githubusercontent.com/sharmapukar217/minor-project-resources/main/{file}\",file, reporthook)\n",
    "    print(\"extracting and processing faces...\")\n",
    "    with ZipFile(file, \"r\") as archive:\n",
    "        namelist = list(filter(lambda x: (x.startswith(\"subject\")), archive.namelist()))\n",
    "        pbar = tqdm(total=len(namelist))\n",
    "        for name in namelist:\n",
    "            if name.startswith(\"subject\"):\n",
    "                classId = name.split(\".\")[0]\n",
    "                archive.extract(name, f\"{TEMP_DIRECTORY}/faces\")\n",
    "                cap = cv2.VideoCapture(f\"{TEMP_DIRECTORY}/faces/{name}\")\n",
    "                ret, image = cap.read()\n",
    "                if ret:\n",
    "                    faces = detector.detect(image, 0.5)[2]\n",
    "                    if len(faces) != 0:\n",
    "                        x, y, w, h = faces[0]\n",
    "                        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                        dir = f\"{FACES_DIRECTORY}/$_{classId}\"\n",
    "                        if not os.path.isdir(dir):\n",
    "                            os.mkdir(dir)        \n",
    "                        cv2.imwrite(f\"{dir}/{name}.pgm\", gray[y:y+h, x:x+w])\n",
    "                        pbar.update(1)\n",
    "        # cleanup\n",
    "        pbar.close()\n",
    "        print(\"cleaning up...\")\n",
    "        \n",
    "        archive.close()\n",
    "        cv2.destroyAllWindows()\n",
    "        shutil.rmtree(f\"{TEMP_DIRECTORY}/faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae22c3-cecc-4489-8435-6145e3055a97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Setup face reader ( for real faces )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4c0d1e92-aba4-412b-a07a-2d22b57286f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the person: sushil_ghimire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't stream the video. exitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@11838.431] global cap_v4l.cpp:1119 tryIoctl VIDEOIO(V4L2:/dev/video0): select() timeout.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "from cuid import cuid\n",
    "\n",
    "detector = get_detector()\n",
    "\n",
    "# read person's name for label\n",
    "name = \"\"\n",
    "while name.strip() == \"\":\n",
    "    name = input(\"Enter the name of the person:\")\n",
    "label = \"$_\" + name.strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "dir = os.path.join(DATA_DIRECTORY, \"faces\", label)\n",
    "if not os.path.isdir(dir):\n",
    "    os.mkdir(dir)\n",
    "else:\n",
    "    letter = input(\"label/name already exists, rewrite? [y/N]\")\n",
    "    letter = letter.lower().strip()\n",
    "    if letter != \"y\":\n",
    "        sys.exit(\"closing the face reader.\")\n",
    "\n",
    "elapsed_time = 0\n",
    "\n",
    "# start camera for face data\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "(count, started) = (0, False)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"can't stream the video. exitting...\")\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    current_time = time.time()\n",
    "    faces = detector.detect(frame.copy(), 0.4)[2]\n",
    "\n",
    "    # only show the first detection\n",
    "    # assuming only single face is present\n",
    "    # while reading the person's face for training\n",
    "    if len(faces) != 0:\n",
    "        cv2.rectangle(frame, faces[0], (255, 255, 255), 5)\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # flip around y axis\n",
    "    fps = int((1 / (current_time - elapsed_time)))\n",
    "    elapsed_time = current_time\n",
    "\n",
    "    cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL)\n",
    "    cv2.putText(frame, f\"fps: {fps}\", (7, 30), cv2.FONT_HERSHEY_DUPLEX, 0.8, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"q\"):\n",
    "        print(\"\\nexitting the face reader...\")\n",
    "        break\n",
    "    elif key == ord(\"c\"):\n",
    "        started =  True\n",
    " \n",
    "    sys.stdout.write(f\"\\rsnapshot: #{count+1}: (q) to exit, (c) to start capturing snapshot.\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if started:\n",
    "        if len(faces) != 0:\n",
    "            # image = cv2.resize(gray[y:y+h, x:x+w], (100, 100), interpolation=cv2.INTER_AREA)\n",
    "            image_path = f\"{dir}/{cuid()}.jpg\"\n",
    "            cv2.imwrite(image_path, gray[y:y+h, x:x+w])\n",
    "            time.sleep(0.05)\n",
    "            count+=1\n",
    "            if INTERACTIVE_READER: started = False\n",
    "        else:\n",
    "            sys.stdout.write(\"\\rNo face found to take snapshot.\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    if count >= TOTAL_IMAGE_SNAPSHOTS:\n",
    "        print(f\"\\ndone taking {count} snapshots. closing...\")\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42b30f-f9ab-4e5b-bcb7-e397ea3ddf8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0776bdfc-f3e2-44e6-8ccd-22626d3633fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def generate_data_set():\n",
    "    id = 0\n",
    "    (labels, faces, ids) = ({}, [], [])\n",
    "    for directory in os.listdir(FACES_DIRECTORY):\n",
    "        if directory.startswith(\"$_\"):\n",
    "            label = directory.split(\"$_\")[1]\n",
    "            for img_file in os.listdir(f\"{FACES_DIRECTORY}/{directory}\"):\n",
    "                if img_file.endswith(\".pgm\"):\n",
    "                    img = Image.open(f\"{FACES_DIRECTORY}/{directory}/{img_file}\").convert(\"L\").resize((112, 92), Image.LANCZOS)\n",
    "                    img_arr = np.array(img,dtype='float32')/255.0\n",
    "                    faces.append(img_arr)\n",
    "                    labels[id] = label\n",
    "                    ids.append(id)\n",
    "                    id = id + 1\n",
    "                    \n",
    "    # return labels, np.asarray(faces), np.asarray(ids)\n",
    "    return labels, faces, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e6c98-74bc-4b8b-b2a5-c5bbec0fa6c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc5395c9-bbd7-40b0-ae7b-8c263fc6d8d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels, faces, ids = generate_data_set()\n",
    "# faces, ids = shuffle(faces, ids)\n",
    "\n",
    "# def plot_random_img(n=10, rows=5 ,cols=5):\n",
    "#     fig=plt.figure(figsize=(rows+5, cols+8))\n",
    "#     for i in range(0, min(n, ids.shape[0])):\n",
    "#         img = faces[i]\n",
    "#         fig.add_subplot(rows, cols, i+1)\n",
    "#         plt.imshow(img,cmap='gray')\n",
    "#         plt.title(f\"#{ids[i]}->{labels.get(ids[i])}\", fontsize=7)\n",
    "#         plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_random_img(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221f08d-47b3-4a48-becf-a3541d0382b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd42354-a224-4101-825b-d2c341daf102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models iddmport Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Dropout,BatchNormalization\n",
    "\n",
    "def Model(input_shape,num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Conv2D(64, (1, 1)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (1, 1)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b66fd5-d95c-45a1-86b8-c769b5744740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 98, 98, 32)        320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 98, 98, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 96, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 96, 96, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 96, 96, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 96, 96, 64)        4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 96, 96, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 96, 96, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 94, 94, 128)       73856     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 128)       0         \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 94, 94, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 47, 47, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 47, 47, 64)        8256      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 47, 47, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 141376)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4524064   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 165)               5445      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 165)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,635,109\n",
      "Trainable params: 4,634,853\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "(165, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from keras.utils import to_categorical\n",
    "from keras import callbacks, backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "K.clear_session()\n",
    "labels, faces,ids = generate_data_set()\n",
    "\n",
    "total_faces = len(set(ids))\n",
    "model = Model((100,100,1), total_faces)\n",
    "\n",
    "# faces = np.array([downsample_image(ab) for ab in faces])\n",
    "faces = np.array(faces)\n",
    "faces = faces[:,:,:,np.newaxis]\n",
    "faces = faces.astype('float32')\n",
    "faces /= 255.\n",
    "\n",
    "\n",
    "# one hot encoding\n",
    "ids = to_categorical(ids)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(faces,ids, test_size = 0.2, random_state = 0)\n",
    "checkpoint = callbacks.ModelCheckpoint('trained_model.h5', monitor='val_acc',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "8f7ba42c-fe8e-449c-8631-a1f808482e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 100, 100, 1) (132, 166)\n",
      "(33, 100, 100, 1) (33, 166)\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "             batch_size=32,\n",
    "             epochs=250,\n",
    "             validation_data=(x_test, y_test),\n",
    "             shuffle=True,callbacks=[checkpoint])\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
